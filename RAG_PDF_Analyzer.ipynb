{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ RAG PDF Analyzer - LOCAL EMBEDDINGS\n",
    "\n",
    "**Works with exhausted API quota!**\n",
    "\n",
    "- âœ… Local embeddings (NO API calls for upload)\n",
    "- âœ… Gemini for Q&A only (1 API call per question)\n",
    "- âœ… Fast uploads (no network delay)\n",
    "- âœ… Works offline for PDF processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers langchain langchain-community langchain-google-genai langchain-classic google-generativeai faiss-cpu PyPDF2 langchain-text-splitters langchain-core torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Step 2: Set API Key (Only for Q&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API Key set (only used for questions)!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'   \n",
    "print(\"âœ… API Key set (only used for questions)!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports ready!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2, warnings, uuid\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… Imports ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Step 4: Local Embeddings Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Local embeddings class ready!\n"
     ]
    }
   ],
   "source": [
    "class LocalEmbeddings(Embeddings):\n",
    "    \"\"\"Local embeddings using sentence-transformers (runs on your PC)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"ğŸ“¥ Loading local embedding model (first time ~100MB download)...\")\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"âœ… Local model loaded!\\n\")\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed multiple texts\"\"\"\n",
    "        return self.model.encode(texts).tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed single query\"\"\"\n",
    "        return self.model.encode([text])[0].tolist()\n",
    "\n",
    "print(\"âœ… Local embeddings class ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 5: RAG Engine with Local Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG Engine ready!\n"
     ]
    }
   ],
   "source": [
    "class RAGEngine:\n",
    "    def __init__(self):\n",
    "        print(\"ğŸ”§ Initializing RAG with LOCAL embeddings...\")\n",
    "        \n",
    "        # LOCAL embeddings (NO API calls!)\n",
    "        self.embeddings = LocalEmbeddings()\n",
    "        \n",
    "        # Gemini for Q&A only\n",
    "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "        )\n",
    "        \n",
    "        self.vector_store = None\n",
    "        self.documents_metadata = {}\n",
    "        print(\"âœ… Ready! Upload uses NO API calls!\\n\")\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        text_parts = []\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            print(f\"ğŸ“– Reading {len(reader.pages)} pages...\\n\")\n",
    "            for i, page in enumerate(reader.pages, 1):\n",
    "                try:\n",
    "                    text = page.extract_text()\n",
    "                    if text and text.strip():\n",
    "                        text_parts.append(text)\n",
    "                        print(f\"   âœ… Page {i}\")\n",
    "                except:\n",
    "                    print(f\"   âŒ Page {i}\")\n",
    "        full = \"\\n\\n\".join(text_parts)\n",
    "        print(f\"\\nâœ… Extracted {len(full)} chars\\n\")\n",
    "        return full\n",
    "    \n",
    "    def add_document(self, pdf_path: str, filename: str = None) -> str:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ğŸ“„ PROCESSING: {filename or pdf_path}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        print(f\"ğŸ“Š Creating {len(chunks)} chunks...\")\n",
    "        print(f\"ğŸ’» Using LOCAL embeddings (no API calls!)\\n\")\n",
    "        \n",
    "        # Prepare metadata\n",
    "        metas = [{\"document_id\": doc_id, \"chunk_id\": i, \"filename\": filename or pdf_path} \n",
    "                for i in range(len(chunks))]\n",
    "        \n",
    "        # Create embeddings locally (FAST!)\n",
    "        if not self.vector_store:\n",
    "            self.vector_store = FAISS.from_texts(chunks, self.embeddings, metas)\n",
    "        else:\n",
    "            new = FAISS.from_texts(chunks, self.embeddings, metas)\n",
    "            self.vector_store.merge_from(new)\n",
    "        \n",
    "        self.documents_metadata[doc_id] = {\n",
    "            \"id\": doc_id,\n",
    "            \"filename\": filename or pdf_path,\n",
    "            \"chunks_count\": len(chunks),\n",
    "            \"created_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"âœ… DONE! {len(chunks)} chunks embedded locally\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        return doc_id\n",
    "    \n",
    "    def query(self, question: str, k: int = 3) -> Dict:\n",
    "        if not self.vector_store:\n",
    "            return {\"error\": \"No documents! Upload a PDF first.\"}\n",
    "        \n",
    "        print(f\"\\nâ“ {question}\\n\")\n",
    "        print(\"ğŸ” Searching locally...\")\n",
    "        \n",
    "        retriever = self.vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "        chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        print(\"ğŸ¤– Asking Gemini (1 API call)...\\n\")\n",
    "        result = chain({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"sources\": len(result.get(\"source_documents\", []))\n",
    "        }\n",
    "\n",
    "print(\"âœ… RAG Engine ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 6: Initialize Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Initializing RAG with LOCAL embeddings...\n",
      "ğŸ“¥ Loading local embedding model (first time ~100MB download)...\n",
      "âœ… Local model loaded!\n",
      "\n",
      "âœ… Ready! Upload uses NO API calls!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag = RAGEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ Step 7: Upload PDF (NO API CALLS!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“„ PROCESSING: My Document\n",
      "==================================================\n",
      "\n",
      "ğŸ“– Reading 8 pages...\n",
      "\n",
      "   âœ… Page 1\n",
      "   âœ… Page 2\n",
      "   âœ… Page 3\n",
      "   âœ… Page 4\n",
      "   âœ… Page 5\n",
      "   âœ… Page 6\n",
      "   âœ… Page 7\n",
      "   âœ… Page 8\n",
      "\n",
      "âœ… Extracted 22834 chars\n",
      "\n",
      "ğŸ“Š Creating 29 chunks...\n",
      "ğŸ’» Using LOCAL embeddings (no API calls!)\n",
      "\n",
      "==================================================\n",
      "âœ… DONE! 29 chunks embedded locally\n",
      "==================================================\n",
      "\n",
      "ğŸ‰ Success! No API calls used!\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"e.pdf\"  # Change to your file\n",
    "\n",
    "try:\n",
    "    doc_id = rag.add_document(pdf_path, \"My Document\")\n",
    "    print(\"ğŸ‰ Success! No API calls used!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Step 8: Ask Questions (Uses Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ What is the main topic of this document?\n",
      "\n",
      "ğŸ” Searching locally...\n",
      "ğŸ¤– Asking Gemini (1 API call)...\n",
      "\n",
      "ğŸ’¡ Answer:\n",
      "The main topic of this document is the **impact and role of print culture and the printing press**, particularly in the context of the **Reformation Movement** and the **transformation of European society**.\n",
      "\n",
      "ğŸ“š Used 3 sources\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the main topic of this document?\"\n",
    "\n",
    "result = rag.query(question)\n",
    "\n",
    "if \"error\" not in result:\n",
    "    print(f\"ğŸ’¡ Answer:\\n{result['answer']}\")\n",
    "    print(f\"\\nğŸ“š Used {result['sources']} sources\")\n",
    "else:\n",
    "    print(result['error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 9: Ask More Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ Summarize the key points\n",
      "\n",
      "ğŸ” Searching locally...\n",
      "ğŸ¤– Asking Gemini (1 API call)...\n",
      "\n",
      "ğŸ’¡ Here are the key points from the provided text:\n",
      "\n",
      "*   **The Silk Routes:** Ancient trade routes connecting China, Central Asia, India, and Europe, facilitating the exchange of goods (like silk, spices, textiles) and the spread of culture, religion, and ideas.\n",
      "*   **Food Travels and Globalization:** The exchange of crops (e.g., potatoes, maize from America to other continents) transformed diets, improved nutrition, led to population growth, and connected societies globally.\n",
      "*   **Impact of the Industrial Revolution:** Started in Britain, it increased production and demand for raw materials, with colonies supplying these materials and serving as markets. Improved transport (railways, steamships) and communication (telegraph) linked global markets, creating a worldwide economic network controlled by industrial powers.\n",
      "*   **Causes of the Great Depression (1929-1934):** Overproduction and falling prices in agriculture and industry, the 1929 US stock market crash, and the collapse of banks, leading to a global fall in trade, employment, and widespread poverty.\n",
      "*   **Migration and Labour Movement:** Millions migrated for work, including indentured laborers (e.g., Indians to the Caribbean), who, despite harsh conditions, contributed to the spread of their culture.\n",
      "*   **Global Agricultural Economy:** Britain became the 'workshop of the world,' with colonies supplying food and raw materials. New agricultural areas (like the USA, Australia) developed using technology and cheap labor.\n",
      "*   **Flow of Capital:** Britain invested surplus capital globally, making London the financial center of the world.\n",
      "*   **The First World War (1914-18):** Fought between the Allies and Central Powers, resulting in massive casualties and economic destruction.\n",
      "*   **Bretton Woods System:** The provided text does not explain the influence of the Bretton Woods System.\n",
      "\n",
      "ğŸ“š Used 3 sources\n"
     ]
    }
   ],
   "source": [
    "# Ask another question\n",
    "result = rag.query(\"Summarize the key points\")\n",
    "if \"error\" not in result:\n",
    "    print(f\"ğŸ’¡ {result['answer']}\")\n",
    "    print(f\"\\nğŸ“š Used {result['sources']} sources\")\n",
    "else:\n",
    "    print(result['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
